# ThinkingTrees Configuration
# OPS (Oracle-Preserving Summarization) Settings

# vLLM Server Configuration
vllm:
  # Available model profiles (all tested with vLLM 0.12.0)
  models:
    # ===== WORKING MODELS (Tested 2024-12-16) =====

    # Qwen3-Next-80B: Hybrid MoE model with NVFP4 quantization
    # Architecture: Qwen3NextForCausalLM | Load time: ~16s | Memory: ~22 GiB
    qwen-80b:
      path: "/mnt/data/models/Qwen3-Next-80B-A3B-Instruct-NVFP4"
      tensor_parallel: 2
      max_model_len: 32768

    # Qwen3-30B-Thinking: Smaller reasoning model with NVFP4
    # Architecture: Qwen3MoeForCausalLM | Load time: ~6.5s | Memory: ~17 GiB
    # NOTE: Required config fix - removed quantization_config from config.json
    qwen-30b-thinking:
      path: "/mnt/data/models/NVFP4/Qwen3-30B-A3B-Thinking-2507-FP4"
      tensor_parallel: 4
      max_model_len: 32768

    # Qwen3-235B: Large instruction model with compressed-tensors NVFP4
    # Architecture: Qwen3MoeForCausalLM | Load time: ~20s | Memory: ~34 GiB (4 GPUs)
    qwen-235b:
      path: "/mnt/data/models/RedHatAI/Qwen3-235B-A22B-Instruct-2507-NVFP4"
      tensor_parallel: 4
      max_model_len: 8192

    # Qwen3-VL-235B: Vision-Language reasoning model with FP8
    # Architecture: Qwen3VLMoeForConditionalGeneration | Load time: ~80s | Memory: ~56 GiB (4 GPUs)
    qwen-vl-235b:
      path: "/mnt/data/models/Qwen/Qwen3-VL-235B-A22B-Thinking-FP8"
      tensor_parallel: 4
      max_model_len: 4096

    # GLM-4.6: Large MoE model with NVFP4 quantization
    # Architecture: Glm4MoeForCausalLM | Load time: ~16s + 2min warmup | Memory: ~47 GiB (4 GPUs)
    # NOTE: CUDAGraph warmup is slow (~2+ min). Use --enforce-eager for faster startup.
    glm-4.6:
      path: "/mnt/data/models/GLM-4.6-NVFP4"
      tensor_parallel: 4
      max_model_len: 8192

    # OLMo-3-32B-Think: Dense reasoning model (full BF16, no quantization)
    # Architecture: Olmo2ForCausalLM | Load time: ~42s | Memory: ~64 GiB (1 GPU)
    olmo-32b-think:
      path: "/mnt/data/models/allenai/Olmo-3-32B-Think"
      tensor_parallel: 1
      max_model_len: 32768

    # NVIDIA Nemotron-3-Nano-30B: MoE model with FP8 quantization
    # Architecture: NemotronForCausalLM | Similar to Qwen3-30B in size
    nemotron-30b-fp8:
      path: "/mnt/data/models/nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-FP8"
      tensor_parallel: 2
      max_model_len: 32768

    # NVIDIA Qwen3-Nemotron-235B GenRM NVFP4: 4-bit quantized reward model
    # Architecture: Qwen3MoE | Memory: ~65 GiB (2 GPUs with TP=2)
    # Use for: Preference scoring, comparison tasks
    genrm-nvfp4:
      path: "/mnt/data/models/nvidia/Qwen3-Nemotron-235B-A22B-GenRM-NVFP4"
      tensor_parallel: 2
      max_model_len: 32768

    # NVIDIA Qwen3-Nemotron-235B GenRM: Generative Reward Model for preference learning
    # Architecture: Qwen3MoE | Memory: ~150 GiB (4-8 GPUs) | BF16
    # Use for: Comparing summaries, generating preference data, RLHF training
    # Special format: Uses response_1 and response_2 roles for comparison
    # Produces: Helpfulness scores (1-5) and ranking score (1-6)
    qwen3-nemotron-genrm:
      path: "/mnt/data/models/nvidia/Qwen3-Nemotron-235B-A22B-GenRM"
      tensor_parallel: 4
      max_model_len: 40000

    # NVIDIA Qwen3-Nemotron-235B GenRM GGUF: Same model with Q4_K_M quantization
    # Architecture: Qwen3MoE | Memory: ~80 GiB (4 GPUs) | GGUF Q4_K_M
    # Much lower memory footprint than BF16 version
    qwen3-nemotron-genrm-gguf:
      path: "/mnt/data/models/GGUF/Qwen3-Nemotron-235B-GenRM-Q4_K_M/Qwen3-Nemotron-235B-A22B-GenRM.i1-Q4_K_M.gguf"
      tensor_parallel: 4
      max_model_len: 32768

  # Server settings
  host: "0.0.0.0"
  port: 8000
  gpu_memory_utilization: 0.90

  # Automatic Prefix Caching (APC)
  # Caches KV cache for repeated prompts (e.g., same rubric/system prompts)
  # Benefits: 30-70% faster prefill for repeated contexts
  # Trade-off: ~5-10% throughput reduction when no shared prefixes
  enable_prefix_caching: true

  # Default model to use
  default: "nemotron-30b-fp8"

# Server URLs
# Can be overridden via environment variables: TASK_MODEL_URL, GENRM_URL
servers:
  task_model_url: "http://localhost:8000/v1"
  genrm_url: "http://localhost:8001/v1"

# Task Configuration
# Tasks define what we do with documents (e.g., scoring, summarization, extraction)
# Available tasks: document_analysis, scoring (+ domain-specific like manifesto_rile)
tasks:
  default: "document_analysis"

# Dataset Configuration
# Datasets define where documents come from (e.g., manifesto corpus, JSONL)
datasets:
  default: "jsonl"
  manifesto:
    data_dir: null
    require_text: true
  jsonl:
    path: null

# Domain Configuration (DEPRECATED - use 'tasks' instead)
# Kept for backward compatibility only
domains:
  default: null  # Use tasks.default instead

  # Manifesto/RILE task configuration (task-specific settings)
  manifesto_rile:
    scale_min: -100
    scale_max: 100
    score_field: "rile_score"
    error_threshold_high: 20.0
    error_threshold_low: 10.0

# LLM Configuration
llm:
  # Provider: vllm, openai, anthropic
  provider: "vllm"
  base_url: "http://localhost:8000/v1"  # Legacy: use servers.task_model_url instead

  # Generation parameters (models support 32768+ context, use 16384 for output to leave room for input)
  max_tokens: 16384
  temperature: 0.3

# Generation settings by pipeline stage
generation:
  summarizer:
    temperature: 0.5
    max_tokens: 16384
    candidate_temperatures: [0.3, 0.5, 0.7, 0.9]
  oracle:
    temperature: 0.3
    max_tokens: 16384
  genrm_judge:
    temperature: 0.6
    top_p: 0.95
    max_tokens: 16384
  comparison_judge:
    temperature: 0.3
    max_tokens: 16384
  synthetic_data:
    temperature: 0.6
    top_p: 0.95
    max_tokens: 16384
  ground_truth:
    temperature: 0.3
    max_tokens: 16384

# Document Chunking
chunking:
  # Maximum tokens per chunk (uses tiktoken)
  max_tokens: 2000
  # Model for token counting
  model: "gpt-4"

# Tree Building
tree:
  # Merge strategy: "binary" for 2-way merge
  merge_strategy: "binary"
  # Enable verbose logging
  verbose: false

# Audit Configuration
audit:
  # Number of nodes to sample per audit
  sample_budget: 10
  # Threshold for flagging discrepancies (0.0 to 1.0)
  discrepancy_threshold: 0.1
  # Prioritize higher levels (more compression)
  prioritize_high_levels: true

# Optimization
optimization:
  # Number of bootstrap examples to use
  bootstrap_examples: 5
  # Maximum optimization iterations
  max_iterations: 3
  # Save intermediate results
  save_checkpoints: true

  # Progress checking (before/after evaluation)
  enable_progress_check: true
  progress_check_samples: null  # null = use all training samples, or specify a number

  # Joint optimization settings (oracle + GenRM)
  optimization_mode: "joint"  # "oracle_only", "genrm_only", "sequential", "joint"
  oracle_weight: 0.7  # Weight for oracle accuracy (only for joint mode)
  genrm_weight: 0.3   # Weight for GenRM consistency (only for joint mode)

# Oracle Function Approximation (Learned Oracle for Reviewing Flagged Nodes)
oracle_func:
  # Training configuration
  max_bootstrapped_demos: 8
  max_labeled_demos: 16
  balance_ratio: 1.0          # Ratio of negative to positive examples
  min_training_examples: 4     # Minimum examples required before training

  # Review thresholds
  confidence_threshold: 0.6    # Minimum confidence to consider a prediction valid
  auto_approve_threshold: 0.8  # Confidence needed to auto-approve (false positive)
  auto_reject_threshold: 0.8   # Confidence needed to auto-reject (true violation)

  # Paths for checkpoints and training data
  checkpoint_dir: "data/oracle_func_checkpoints"
  training_data_path: "data/oracle_func_training.json"

  # Continuous learning
  retrain_threshold: 10        # New examples needed before auto-retraining

# Paths
paths:
  data_dir: "data"
  raw_dir: "data/raw"
  processed_dir: "data/processed"
  trees_dir: "data/trees"

# Speculative Decoding Configuration
# Uses a smaller "draft" model to propose tokens, verified by larger "target" model
# Result: 1.5-3x faster generation with identical outputs
speculative:
  # Default settings (can be overridden by presets)
  num_speculative_tokens: 5         # Tokens to draft per step (5-8 typical)

  # Preset combinations for different use cases
  presets:
    # Training: Large target model + small draft for faster DSPy optimization
    training:
      target: "qwen-80b"            # Large model for optimization quality
      draft: "qwen-30b-thinking"    # Fast draft model
      enabled: true
      num_speculative_tokens: 5

    # Inference: Just the small model with DSPy-optimized prompts
    # The optimized prompts/demos from training help the small model perform well
    inference:
      target: "qwen-30b-thinking"   # Small model with optimized prompts
      draft: null                   # No speculative decoding needed
      enabled: false

    # Heavy training: 235B target for maximum quality (requires 4+ GPUs)
    training-heavy:
      target: "qwen-235b"
      draft: "qwen-30b-thinking"
      enabled: true
      num_speculative_tokens: 5

# Logging
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
