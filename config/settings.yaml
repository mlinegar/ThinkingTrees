# ThinkingTrees Configuration
# OPS (Oracle-Preserving Summarization) Settings

# vLLM Server Configuration
vllm:
  # Available model profiles (all tested with vLLM 0.12.0)
  models:
    # ===== WORKING MODELS (Tested 2024-12-16) =====

    # Qwen3-Next-80B: Hybrid MoE model with NVFP4 quantization
    # Architecture: Qwen3NextForCausalLM | Load time: ~16s | Memory: ~22 GiB
    qwen-80b:
      path: "/mnt/data/models/Qwen3-Next-80B-A3B-Instruct-NVFP4"
      tensor_parallel: 2
      max_model_len: 32768

    # Qwen3-30B-Thinking: Smaller reasoning model with NVFP4
    # Architecture: Qwen3MoeForCausalLM | Load time: ~6.5s | Memory: ~17 GiB
    # NOTE: Required config fix - removed quantization_config from config.json
    qwen-30b-thinking:
      path: "/mnt/data/models/NVFP4/Qwen3-30B-A3B-Thinking-2507-FP4"
      tensor_parallel: 1
      max_model_len: 32768

    # Qwen3-235B: Large instruction model with compressed-tensors NVFP4
    # Architecture: Qwen3MoeForCausalLM | Load time: ~20s | Memory: ~34 GiB (4 GPUs)
    qwen-235b:
      path: "/mnt/data/models/RedHatAI/Qwen3-235B-A22B-Instruct-2507-NVFP4"
      tensor_parallel: 4
      max_model_len: 8192

    # Qwen3-VL-235B: Vision-Language reasoning model with FP8
    # Architecture: Qwen3VLMoeForConditionalGeneration | Load time: ~80s | Memory: ~56 GiB (4 GPUs)
    qwen-vl-235b:
      path: "/mnt/data/models/Qwen/Qwen3-VL-235B-A22B-Thinking-FP8"
      tensor_parallel: 4
      max_model_len: 4096

    # GLM-4.6: Large MoE model with NVFP4 quantization
    # Architecture: Glm4MoeForCausalLM | Load time: ~16s + 2min warmup | Memory: ~47 GiB (4 GPUs)
    # NOTE: CUDAGraph warmup is slow (~2+ min). Use --enforce-eager for faster startup.
    glm-4.6:
      path: "/mnt/data/models/GLM-4.6-NVFP4"
      tensor_parallel: 4
      max_model_len: 8192

    # OLMo-3-32B-Think: Dense reasoning model (full BF16, no quantization)
    # Architecture: Olmo2ForCausalLM | Load time: ~42s | Memory: ~64 GiB (1 GPU)
    olmo-32b-think:
      path: "/mnt/data/models/allenai/Olmo-3-32B-Think"
      tensor_parallel: 1
      max_model_len: 32768

  # Server settings
  host: "0.0.0.0"
  port: 8000
  gpu_memory_utilization: 0.90

  # Default model to use
  default: "qwen-80b"

# LLM Configuration
llm:
  # Provider: vllm, openai, anthropic
  provider: "vllm"
  base_url: "http://localhost:8000/v1"

  # Generation parameters
  max_tokens: 2000
  temperature: 0.3

# Document Chunking
chunking:
  # Maximum tokens per chunk (uses tiktoken)
  max_tokens: 2000
  # Model for token counting
  model: "gpt-4"

# Tree Building
tree:
  # Merge strategy: "binary" for 2-way merge
  merge_strategy: "binary"
  # Enable verbose logging
  verbose: false

# Audit Configuration
audit:
  # Number of nodes to sample per audit
  sample_budget: 10
  # Threshold for flagging discrepancies (0.0 to 1.0)
  discrepancy_threshold: 0.1
  # Prioritize higher levels (more compression)
  prioritize_high_levels: true

# Optimization
optimization:
  # Number of bootstrap examples to use
  bootstrap_examples: 5
  # Maximum optimization iterations
  max_iterations: 3
  # Save intermediate results
  save_checkpoints: true

# Oracle Function Approximation (Learned Oracle for Reviewing Flagged Nodes)
oracle_func:
  # Training configuration
  max_bootstrapped_demos: 4
  max_labeled_demos: 8
  balance_ratio: 1.0          # Ratio of negative to positive examples
  min_training_examples: 4     # Minimum examples required before training

  # Review thresholds
  confidence_threshold: 0.6    # Minimum confidence to consider a prediction valid
  auto_approve_threshold: 0.8  # Confidence needed to auto-approve (false positive)
  auto_reject_threshold: 0.8   # Confidence needed to auto-reject (true violation)

  # Paths for checkpoints and training data
  checkpoint_dir: "data/oracle_func_checkpoints"
  training_data_path: "data/oracle_func_training.json"

  # Continuous learning
  retrain_threshold: 10        # New examples needed before auto-retraining

# Paths
paths:
  data_dir: "data"
  raw_dir: "data/raw"
  processed_dir: "data/processed"
  trees_dir: "data/trees"

# Logging
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
